# 1. 什么是爬虫

## 1.1 爬虫Spider的概念

爬虫用于爬取数据，又称之为**数据采集程序**。

爬取的数据来源于网络，网络中的数据可以是由**Web服务器**（Nginx/Apache）、数据库服务器（MySQL、Redis）、索引库（ElastichSearch）、大数据（Hbase/Hive）、视频/图片库（FTP/）、云存储（OSS）等提供。

爬取数据是公开的、非盈利的。



## 1.2 python爬虫

使用python编写的爬虫脚本（程序）可以完成定时、定量、指定目标（Web站点）的数据爬取。主要使用多(单) 线程 / 进程、网络请求库、数据解析、数据存储、任务调度等相关技术。

python爬虫工程师，可以完成接口测试、功能性测试、性能测试和集成测试。

# 2. 爬虫与Web后端服务之间的关系

爬虫使用网络请求库，相当于客户端请求，Web后端服务根据请求响应数据。

服务器：（Django、Flask、Tornado）都是来实现WSGI（Web Service Gateway Interface）协议的。

> 什么是WSGI？

客户端：浏览器、手机。都是基于WebKit

请求都是HTTP请求

响应都是HTTP响应

请求头（header）和请求体（body）由一行空行分隔开，一个请求报文的例子如下：

```
GET / HTTPs/1.1
HOST: www.baidu.com
Content-Type: application/json
Content-Length: 24

{"name":"jack","phone":"13311002200"}
```

响应报文

```
HTTP/1.1 200 OK
Content-Type: text/html,charset=utf-8
Content-Length:300

<html>
  <h1>hello</h1>
</html>
```

爬虫使用网络请求库，相当于客户端请求，Web后端服务根据请求响应数据。

爬虫就是向Web服务器发起HTTP请求，正确地接收响应数据，然后根据数据的类型（Content-Type）进行数据的解析及存储。

爬虫程序在发起请求前，需要伪造一个浏览器（User-Agent指定请求头）然后再向服务器发起请求，响应200的成功率会高很多。

# 3. python爬虫技术的相关库

网络请求：

- urllib
  - urllib3 需要额外安装，是对urllib的封装，是第三方库
- requests
- selenium（UI自动测试、动态js渲染）
- appium（手机App的爬虫或UI测试）

数据解析：

- re正则，正则中的量词
- xpath
- bs4
- json

> json序列化与反序列化：
>
> 把列表或字典转换成json格式的**字符串**，json中字符串一定是双引号的。

数据存储：

- pymysql
- mongodb
- elasticsearch

多任务库：

- 多线程（threading）、线程队列（queue）

- 协程（asynio、gevent、eventlet）

  > 多线程是由全局解释器锁来调度程序的运行，给程序划分好时间片。
  >
  > 而协程是自己调度自己，当一个程序阻塞时，运行其他程序
  >
  > 项目中那块运用到进程：启动一个服务器时就用到进程。或者写一个定时任务，可以用到进程。

爬虫框架

- scrapy
- scrapy-redis分布式（多机爬虫）

# 4. 常见反爬策略

- UA策略（User-Agent）
- 登录限制（Cookie）策略
- 请求频次（IP代理）策略
- 验证码（图片-云打码，文字或物件图片选择、滑块验证码）
- 动态js（Selenium/Splash/找api接口）策略

# 5. 爬虫库urllib

https://docs.python.org/3/library/urllib.request.html?highlight=urllib

## 5.1 urllib.request模块

### 简单的请求

`urlopen()`不传data默认为GET请求

`resp`是`http.client.HTTPResponse`类对象

属性：

- `code` 状态码
- `headers`响应头，通过`resp.getheaders()`
- `msg` 响应状态
- `url`
- `status`

方法：

- read()
- readlines()，一行一行的读，列表的一个元素是一行

读网络数据阻塞的原因，缓存槽没写满。

todo: 收集http协议的报文头的key

```python
"""
urllib.request.urlopen(url)  # 发起get请求
urllib.parse.quote()  # 将中文进行url编码
urllib.request.urlretrieve(url, filename)  # 下载url保存到filename
"""

from urllib.request import urlopen, urlretrieve
from urllib.parse import quote

import ssl
ssl._create_default_https_context = ssl._create_unverified_context

def search_baidu(wd="dota"):
    # 网络资源的接口(URL)
    url = "https://www.baidu.com/s?wd=%s"
    resp = urlopen(url % quote(wd))
    assert resp.code == 200
    print('请求成功')
    
    # 读取响应的数据
    bytes_ = resp.read()
    
    # 当对象进入上下文时，调用对象的哪个方法
    # f变量接收open()函数返回对象的__enter__()返回的结果
    # f = open(..).__enter__()
    # 当对象退出上下文时，调用对象的哪个方法
    with open('%s.html' % wd, 'wb') as f:
        f.write(bytes_)

if __name__ == '__main__':
    search_baidu()
```

### 带请求头的请求

可以看到，返回的结果很少，这是因为百度识别到了你是爬虫

不给你展示太多信息，下面需要用到一个请求类`urllib.request.Request`

参数一览：

```python
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
```



```python
from urllib.parse import quote
from urllib.request import urlopen, urlretrieve, Request
import ssl

ssl._create_default_https_context = ssl._create_unverified_context


def search_baidu(wd="dota"):
    # 网络资源的接口(URL)
    url = "https://www.baidu.com/s?wd=%s"
    # 生成请求对象，封装请求的url和header
    request = Request(url % quote(wd),
                       headers={
                           "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; "
                                         "x64; rv:84.0) Gecko/20100101 Firefox/84.0"
                       })
    response = urlopen(request)
    assert response.code == 200
    print('请求成功')

    # 读取响应的数据
    bytes_ = response.read()

    # 当对象进入上下文时，调用对象的哪个方法
    # 当对象退出上下文时，调用对象的哪个方法
    with open('%s.html' % wd, 'wb') as f:
        f.write(bytes_)


if __name__ == '__main__':
    search_baidu()
```

### 下载图片

`urlretrieve()`只能下载简单的网页

```python
def download_img(url):
    # 从url中获取文件名
    filename = url[url.rfind('/')+1:]
    req = Request(url % quote(wd),
                       headers={
                           "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; "
                                         "x64; rv:84.0) Gecko/20100101 Firefox/84.0"
                       })
    # urlretrieve(req, filename)  只能下载普通的页面
    res = urlopen(req)
    with open(filename, 'wb') as f:
        f.write(res.read())
```

## 5.2 urllib.parse模块

此模块两个核心的函数：

- quote() 仅对中文字符串进行url编码
- urlencode() 可以对针对一个字典中所有的values进行编码，然后转成`key=value&key=value`的字符串

```python
import urllib.parse

text = "hello你好2021"
print(urllib.parse.quote(text))
# hello%E4%BD%A0%E5%A5%BD2021
```



一个汉字=三个字节

```python
import urllib.parse
query = dict(name="小明", age=18)
print(urllib.parse.urlencode(query))
# name=%E5%B0%8F%E6%98%8E&age=18

query = [('name', 'lily'), ('age', 18)]
print(urllib.parse.urlencode(query))
# name=lily&age=18
```

小题目：

给出16进制的字符列表，请排序

```python
arr = ['17a', '1', '16', '1f', '87']
ans = sorted(arr, key=lambda x: int(x, 16))
# ['1', '16', '1f', '87', '17a']
```

### 百度翻译

```python
"""
应用：百度翻译
- urllib.request.Request
- urllib.request.urlopen()
- urllib.parse.urlencode()
- 发起post请求
"""
import json
from urllib.request import Request, urlopen
from urllib.parse import urlencode

# 请求的api接口
url = 'https://fanyi.baidu.com/sug'


def fanyi(kw):
    data = {
        'kw': kw
    }

    # Request() 中的data参数是byte类型
    req = Request(url,
                  data=urlencode(data).encode('utf-8'))

    resp = urlopen(req)
    assert resp.code == 200

    bytes_ = resp.read()
    # content_type = resp.getheader('Content-Type')

    return json.loads(bytes_.decode('utf8'))


if __name__ == '__main__':
    print(fanyi('pear'))

```

### 百度贴吧复杂的GET请求

由用户录入吧名以及要查看的起始页和结束页，爬取该贴吧的对应信息，每一页保存为一个html文件

```python
"""
复杂的GET请求，多页面请求下载
- urllib.request.Request
- urllib.request.urlopen()
- urllib.parse.urlencode()
- 发起post请求
"""
import json
from urllib.request import Request, urlopen
from urllib.parse import urlencode

# 请求的api接口
url = 'https://www.baidu.com/s?'

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; "
                  "x64; rv:84.0) Gecko/20100101 Firefox/84.0",
    "Cookie": "BIDUPSID=F381695296D77105450FCCED45B1C0F6; PSTM=1610010492; BAID"
              "UID=F381695296D77105550CFBEAD0C9EC1B:FG=1; BDUSS=U1OUmtLaERsSVVZ"
              "WEM4SHcwSEwzU2lxZHo3Z1VienJCU3ctTE1Pb2lPU1FVVFpnRVFBQUFBJCQAAAAA"
              "AAAAAAEAAAAHPxQzTGlnaHRzYXZld29ybGQAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
              "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJDEDmCQxA5gS; BDUSS_BFESS=U"
              "1OUmtLaERsSVVZWEM4SHcwSEwzU2lxZHo3Z1VienJCU3ctTE1Pb2lPU1FVVFpnRV"
              "FBQUFBJCQAAAAAAAAAAAEAAAAHPxQzTGlnaHRzYXZld29ybGQAAAAAAAAAAAAAAA"
              "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJDEDmCQxA5gS;"
              " __yjs_duid=1_cd345af886cd975053c27868a1c506851611747911470; Hm_"
              "lvt_64ecd82404c51e03dc91cb9e8c025574=1611907962; Hm_lpvt_64ecd82"
              "404c51e03dc91cb9e8c025574=1611907962; REALTIME_TRANS_SWITCH=1; F"
              "ANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_"
              "PREFER_SWITCH=1; ab_sr=1.0.0_NmQ3NWExNjdlYTI2ZGNiNjgxODk1OGJmMmQ"
              "zNDk1NDVmYmI0N2YzMWU2MTU0ODdiZmE0YTcxMDkwZGRjMTEyZThhZjJjYjU0NjZ"
              "lOGQ5NTFiMDdiZTY0YTRhYmJlYWNm",
    "X-Requested-With": "XMLHttpRequest"
}


def main(wd):
    for page in range(10):
        params = {
            'wd': wd,  # xx
            'pn': page * 10  # 第几页
        }
        page_url = url + urlencode(params)

        resp = urlopen(Request(page_url, headers=headers))
        assert resp.code == 200

        bytes_ = resp.read()
        with open('pages/%s-%s.html' % (wd, page), 'wb') as f:
            f.write(bytes_)

        print('保存第%s页成功' % (page + 1))


if __name__ == '__main__':
    main('python')

```

### 肯德基餐厅信息查询

在首页的最下方可以找到这个页面

http://www.kfc.com.cn/kfccda/storelist/index.aspx

打开F12可以看到是用

http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname

发请求的

请求头简单查看

```
X-Requested-With: XMLHttpRequest

User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36

Content-Type: application/x-www-form-urlencoded; charset=UTF-8

Cookie: route-cell=ksa; SERVERID=ea0e5114dce400194a5b71a7eef453ee|1611914259|1611914145
```

Form Data一览

```
cname: 上海
pid: 
pageIndex: 1
pageSize: 10
```

响应一览

```json
{
	"Table": [{
		"rowcount": 455
	}],
	"Table1": [{
		"rownum": 1,
		"storeName": "茂名",
		"addressDetail": "吴江路269号2层",
		"pro": "店内参观,礼品卡,手机点餐,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 2,
		"storeName": "翔川",
		"addressDetail": "妙镜路1118号E号商铺",
		"pro": "Wi-Fi,点唱机,店内参观,礼品卡,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 3,
		"storeName": "动力南广场（汇金奥特莱斯B1层）",
		"addressDetail": "石龙路750-3号上海南站地下商场南馆",
		"pro": "礼品卡",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 4,
		"storeName": "江苏",
		"addressDetail": "江苏路398号1、2层（舜元弘基天地1楼）",
		"pro": "Wi-Fi,店内参观,礼品卡,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 5,
		"storeName": "威宁",
		"addressDetail": "天山路352号101和201",
		"pro": "Wi-Fi,点唱机,店内参观,礼品卡,生日餐会,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 6,
		"storeName": "思贤",
		"addressDetail": "思贤路778--780号",
		"pro": "Wi-Fi,礼品卡,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 7,
		"storeName": "惠乐",
		"addressDetail": "惠南镇人民西路955号",
		"pro": "Wi-Fi,礼品卡,生日餐会,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 8,
		"storeName": "柳州",
		"addressDetail": "沪闵路9001号上海南站站厅层",
		"pro": "礼品卡,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 9,
		"storeName": "真北",
		"addressDetail": "桃浦路328号",
		"pro": "Wi-Fi,店内参观,礼品卡,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}, {
		"rownum": 10,
		"storeName": "马陆弘基",
		"addressDetail": "马陆镇沪宜公路2398/2400号",
		"pro": "Wi-Fi,礼品卡,生日餐会,溯源",
		"provinceName": "上海市",
		"cityName": "上海市"
	}]
}
```

使用python爬取

```python
"""
根据城市查找kfc店铺信息
"""
import json
from urllib.request import urlopen, Request
from urllib.parse import urlencode

url = "http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                  " (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36",
    "Cookie": "route-cell=ksa; SERVERID=ea0e5114dce4001"
              "94a5b71a7eef453ee|1611914259|1611914145",
    "X-Requested-With": "XMLHttpRequest",
}


def main(city):
    params = {
        "cname": "上海",
        "pid": "",
        "pageIndex": 1,
        "pageSize": 10,
    }
    req = Request(url,
                  data=urlencode(params).encode('utf8'),
                  headers=headers)

    resp = urlopen(req)
    return json.loads(resp.read())


if __name__ == '__main__':
    res = main('上海')
	print(res)
```





### 豆瓣动作电影排行榜



## 5.3 Handler处理器

`urllib.request.urlopen()`不能定制请求头

`urllib.request.Request`可以定制请求头

handler可以定制更高级的请求头

如果有对数据需要一些保存，代理等功能，就需要处理器。

提供了一个处理类，帮助我存储与服务端交互的必要数据，比如要存cookie

```python
# 创建Handler对象
handler = urllib.request.HTTPHandler()
# 创建opener对象
opener = urllib.request..build_opener(handler)
# 创建Request对象
# 发送Request请求
opener.open(req)
```



例1：

```python
"""
模拟浏览器，增加不同的处理器
"""
import json
from urllib.request import urlopen, Request, build_opener, HTTPHandler
from urllib.parse import urlencode
from collections import namedtuple

# 声明 了一个有命名的元组类
Response = namedtuple("Response",
                      field_names=['headers', 'code', 'encoding',
                                   'text', 'body'])


def get(url):
    opener = build_opener(HTTPHandler())
    resp = opener.open(url)
    # 要求返回某一个类对象
    # 带有headers->dict, code-> int, text->文本, body字节码等

    headers = dict(resp.getheaders())
    code = resp.code
    try:
        encoding = headers['Content-Type'].split("=")[-1]
    except:
        encoding = 'utf-8'
    body = resp.read()
    text = body.decode(encoding)

    # 声明一个类
    return Response(headers, code, encoding, body, text)


if __name__ == '__main__':
    resp = get("http://www.baidu.com")
    print(resp.code)

```

创建后不能修改`namedtuple`类的属性

例2：模拟处理Cookie

```python
# 1. 创建一个CookieJar对象
from http.cookiejar import CookieJar
cookie = cookiejar.CookieJar()

# 2. 使用cookiejar对象创建一个handler对象
handler = urllib.request.HTTPCookieProcessor(cookie)
# 3. 使用handler创建一个opener
opener = urllib.request.build_opener(handler)
# 4. 通过opener登录

# 5. handler会自动保存登录之后的cookie
```

查找免费代理，http://www.66ip.cn/areaindex_2/1.html



这个代码暂时跑不起来

```python
"""
多个urllib的请求处理器
- Cookie
- Proxy
- Http
"""
from urllib.parse import urlencode
from urllib.request import build_opener, HTTPHandler, HTTPCookieProcessor, \
    ProxyHandler, Request
from http.cookiejar import CookieJar

cookie = CookieJar()
opener = build_opener(HTTPHandler(),
                      HTTPCookieProcessor(cookie),
                      # ProxyHandler(proxies={
                      #     "http": "http://proxy-ip:port",
                      #
                      # })
                      )
post_url = "http://www.renren.com/ajaxLogin/login?1=1" \
           "&uniqueTimestamp=20182122180"

data = {
    'rkey': '1c7df63368df7ce73c234de26178ecll',
    'password': '19870115',
    'origURL': 'http://www.renren.com/home',
    'key_id': '1',
    'icode': '',
    'f': 'http://www.renren.com/224549540',
    'email': 'dqsygcz@126.com',
    'domain': 'renren.com',
    'captcha_type': 'web_login',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
                  ' AppleWebKit/537.36',
    'Referer': 'http://www.renren.com/SysHome.do'
}
request = Request(post_url, urlencode(data).encode('utf8'), headers)

resp = opener.open(request)  # http.client.HTTPResponse

bytes_ = resp.read()
print(bytes_.decode('utf8'))


```

# 6. 补课

## 6.1 python 上下文

### 什么是上下文

任意的python对象，都可以使用上下文环境，使用**with**关键字。

上下文是代码片段的区域，当对象进入上下文环境时，python解释器会调用对象的`__enter()__`，当对象退出上下文环境时，会调用对象的`__exit__`方法。

### 为什么使用

对象在使用上下文环境时，为了确保对象正确的**释放资源**，避免出现**内存遗漏**。

> 释放资源：
>
> - 文件操作对象`open`
> - 数据库的连接`conn`
> - 线程锁`Lock`



### 如何使用

**step1 重写类的方法**

```python
class A:
    def __enter__(self):
        # 进入上下文时被调用
        # 必须返回一个相关的对象
        print('enter')
        return 'disen'
    def __exit__(self, except_type, val, tb):
        # except_type 如果存在对象时，表示为异常类的实例对象
        # val 异常消息message
        # tb 异常的跟踪栈
        # 返回True 表示存在异常不向外抛出
        # 返回False 表示存在异常，向外抛出
        print('exit')
        return False
```



**step2 在with中使用**

不发生异常的情况

```python
with A() as a:
    print(a)
"""
enter
jack
exit
"""
```

发生异常的情况

```python
with A() as a:
    print(b)
    
# NameError: name 'b' is not defined
```

如果将`__exit__()`中的返回值改成`True`就不会报异常

## 6.2 Dao设计

DAO(Data Access Object) 数据访问对象

只是一种设计思想，目的是简化对数据库层操作。针对实体类（数据模型类）对象，封装一套与数据库操作的SDK(Software Develope Kit)。

合理的DAO的SDK的设计：

```
|- dao (基础数据库操作模块)
  |- __init__.py
  |- base.py
|- entity (实体类模块)
  |- user
  |- order
|- db (具体数据操作)
  |- user_db.py
  |- order_db.py
```

`dao/__init__.py`

```python
# dao/__init__.py

import pymysql
from pymysql.cursors import DictCursor


class Connection():
    def __init__(self):
        self.conn = pymysql.Connect(host='localhost',
                                    port=3306,
                                    user='root',
                                    password='root',
                                    db='stu',
                                    charset='utf8')

    def __enter__(self):
        # DictCursor 针对查询结果进行字典转换
        return self.conn.cursor(cursor=DictCursor)

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type:
            self.conn.rollback()  # 回滚事务
            # 日志收集异常信息，上报给服务器
        else:
            self.conn.commit()  # 提交事务

    def close(self):
        try:
            self.conn.close()
        except:
            pass
```

`dao/base.py`

```python
from dao import Connection


class BaseDao:
    def __init__(self):
        self.conn = Connection()

    def query(self,table_name, *columns, where=None, whereargs=None):
        sql = "SELECT %s from %s "
        sql = sql % (','.join(columns), table_name)

        if where:
            sql += where

        with self.conn as c:
            if whereargs:
                # args 可以是 tuple 与sql 中的%s 对应
                #      可以是 dict 与 sql 中 %(<name>)s 对应
                c.execute(sql, whereargs)
            else:
                c.execute(sql)

            ret = c.fetchall()  # list=[<dict>, <dict>]
        return ret

    def save(self, table_name, instance):
        pass

    def update(self, table_name, instance, where, whereargs):
        pass

    def delete(self, table_name, where, whereargs):
        pass

```

`db/stu_db.py`

```python
from dao.base import BaseDao

from entity.student import Student


class StuDao(BaseDao):
    def query(self, table_name='student', columns=('sn', 'name'),
              where=None, whereargs=None):
        ret = super(StuDao, self).query(table_name, columns,
                                        where=where, whereargs=whereargs)

        return [
            Student(item['sn'], item['name'], item['age'], item['sex'])
            for item in ret
        ]


if __name__ == '__main__':
    dao = StuDao()
    print(dao.query(where='where sex=%s', whereargs=('女',)))
    print(dao.query(where='where sex=%(sex)s', whereargs={'sex': '男'}))

```

`entity/student.py`

```python
class Student:
    def __init__(self, sn, name, age, sex):
        self.sn = sn
        self.name = name
        self.age = age
        self.sex = sex

    def __str__(self):
        return self.name
```

# 7. 总结

核心网络请求库：urllib库

- `urllib.request` 模块

  - `urlopen(url | request: Request, data=None)` data是bytes类型

    ```python
    # 常用法
    Request(url, data=urlencode(data).encode())
    ```

  - `urlretrieve(url, filename)` 下载url的资源到指定的文件

  - `build_opener(*handlder)`构造浏览器对象

    - `opener.open(url|request [, data])`发起请求

  - `Request`构造请求的类

  - `HTTPHandler` HTTP协议请求处理器

  - `ProxyHandler(proxies={'http', 'http://x.x.x.x:x'})` 代理处理器

  - `HTTPCookieProcessor(Cookiejar())`

    - `http.cookiejar.Cookiejar` 类

- `urllib.parse` 模块

  - `quote(txt)` 将字符串转成url编码。
  - `urlencode(query: dict)`将参数的字典转成url编码。
    - `key=val&key=val`
    - 即以application/x-www-form-urlencoded 作为url编码类型