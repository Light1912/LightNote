# requests库

https://requests.readthedocs.io/zh_CN/latest/

requests也是一个网络请求库，基于urllib和urllib3封装的便捷使用的

安装

```
pip install requests -i https://mirrors.aliyun.com/pypi/simple
```

## 核心函数

- `requests.request()`所有请求方法的基本方法

  以下是request()方法的参数说明

  - `method`： 指定请求方法 GET / POST / PUT DELETE
  - `url`： 请求的资源接口（API），在RESTful规范中即是URI（统一资源标识符）

  - `params`：dict，用于GET请求的查询参数（Query String Params）

  - `data`：dict，用于 POST / PUT / DELETE 请求的表单参数（Form Data）

  - `json`：dict，用于上传json数据的参数，封装到body（请求体）中。请求头的Content-Type默认设置为 `application/json`

  - `headers`：dict

  - `cookies`：dict

  - `files`：dict，`{'filename': fileobj}`。也可以是tuple：`(filename, fileobj[, content_type, custom_headers])` 。

    指定files用于上传文件，一般用post请求，默认请求头的`Content-Type: multipart/form-data`

  - `auth`：tuple，用于授权的用户名和口令，形式`('username', 'pwd')`

  - `timeout`：超时时间，单位是秒

  - `allow_redirects`：

  - `proxies`：d

  - ict，设置代理

  - `verify`：

- `requests.get()` 查询数据

  - url
  - params
  - json
  - headers/coodies/auth

- `requests.post()` 上传 / 添加数据

  - url
  - data：可以与files组合，但是不能和json组合
  - json
  - files
  - headers/cookies/auth

- `requests.put()` 修改 / 更新数据

- `requests.delete()` 删除数据

- `requests.patch()`**不建议使用**，涉及HTTP幂等性的问题，可能会出现重复处理。

## 返回值

以上的请求方法返回的对象类型都是：`requests.Response`

```python
import requests
resp = requests.get('http://www.baidu.com')
```

常用属性：

- `encoding`：响应数据集的字符编码

- `ok`：bool，是否连接成功

- `status_code`：状态码，如果是200表示成功

- `text`：得到的结果，如果是get请求，会得到html源码

- `url`：请求的地址

- `content`，响应的字节数据

- `headers`：dict 响应头

- `cookies`,RequesetsCookieJar

  把cookies转成字符串

  ```python
  ";".join(["%s=%s" % (c.name, c.value) for c in resp.cookies])
  ```

- `connection`，HTTPAdapter，可以通过源码学习设计模式

- `elapsed`：`{timedelta} 0:00:09.567540`

- `history`

- `is_permanent_redirect`

- `is_redirect`

- `links`

- `next`

- `raw`：`HTTPResponse`

- `reason`

- `request`

常用方法

- `json()`如果响应数据类型为`application/json`，则将响应的数据进行反序列化成python的list或dict对象。



# xpath数据解析

http://www.zvon.org/xxl/XPathTutorial/General_chi/examples.html

https://www.cnblogs.com/lone5wolf/p/10905339.html

> xpath属于xml/html解析数据的一种方式，基于元素（Element）的树形结构（Node > Element）
>
> 选择某一个元素时，根据元素的路径选择，如`/html/head/title`获取`<title>`标签。

安装包

```
pip install lxml
```



## 绝对路径

从根标签开始，按tree结构依次向下查询。

如 `/html/body/table/tbody/tr`

## 相对路径

相对路径可以有以下写法

- 相对于整个文档

  `//img`：查找出文档中所有的`<img>`标签

- 相对于当前结点

  假设当前结点为`<table>`查找它的所有的`<img>`标签：`.//img`

## 数据提取

- 提取文本

  `//title/text()`

- 提取属性

  `//img/@href`

## 位置条件

获取第一个标签 

```
//meta[1]//@content
```

获取最后一个标签

```
//meta[last()]//@content
```

获取倒数第二个标签

```
//meta[position()-2]//@content
```

获取前三个标签

```
//meta[position()<3]//@content
```

同时获取两个标签

`|`的前后是两个完整且独立的xpath路径

```
//title | //price
```



## 属性条件

查找class为`circle-img`的`<img>`标签

```
//img[@class='circle-img']
```

条件属性

```
//li[@class="" and @name=""]
```

## 模糊条件

```
//div[contains(@class, "page")]
```

第一个class的属性值为box的div标签

```
//div[starts-with(@class, "box")]
```

```
//div[ends-with(@class, "box")]
```



## 轴

用法举例

```
xpath('//div[@class="hello"]/parent::*')
```



| 轴名称            | 表达式                            | 描述                                         |
| ----------------- | --------------------------------- | -------------------------------------------- |
| ancestor          | `xpath('./ancestor::*')`          | 选取当前节点的所有先辈节点（父、祖父）       |
| ancestor-or-self  | `xpath('./ancestor-or-self::*')`  | 选取当前节点的所有先辈节点以及节点本身       |
| attribute         | `xpath('./attribute::*’)`         | 选取当前节点的所有属性                       |
| child             | `xpath('./child::*')`             | 返回当前节点的所有子节点                     |
| descendant        | `xpath('./descendant::*')`        | 返回当前节点的所有后代节点（子节点、孙节点） |
| following         | `xpath('./following::*')`         | 选取文档中当前节点结束标签后的所有节点       |
| following-sibing  | `xpath('./following-sibing::*')`  | 选取当前节点之后的兄弟节点                   |
| parent            | `xpath(‘./parent::*')`            | 选取当前节点的父节点                         |
| preceding         | `xpath('./preceding::*')`         | 选取文档中当前节点开始标签前的所有节点       |
| preceding-sibling | `xpath('./preceding-sibling::*')` | 选取当前节点之前的兄弟节点                   |
| self              | `xpath('./self::*')`              | 选取当前节点                                 |

## Element对象



# 案例

## 安居客

https://shanghai.anjuke.com/community/

爬取并解析安居客房源信息

- 基于requests库实现网络请求
- 基于xpath实现数据提取

```python
import requests
from lxml import etree

url = 'https://shanghai.anjuke.com/community/#'
headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36',
    'cookie': 'sessid=78D34857-A4C9-4DAF-BF37-D3442ECCF429; aQQ_ajkguid=096E067'
              'E-F572-47B5-A283-322E086C3ED8; ctid=11; obtain_by=2; twe=2; id58'
              '=e87rkGAaNzeQarNrOSC5Ag==; 58tj_uuid=40a5c3fd-430f-4522-8f83-cb2'
              '3747044d6; _ga=GA1.2.866920914.1612330951; _gid=GA1.2.1548943231'
              '.1612330951; utm_source=; spm=; new_uv=2; init_refer=; _gat=1; a'
              'ls=0; new_session=0; xxzl_cid=1866cb3b79b74d619ee66ae8c9b427ef; '
              'xzuid=8ee4df61-fdf9-46e1-a447-d71bcf61307d'
}


def get(url):
    resp = requests.get(url, headers=headers)
    if resp.status_code == 200:
        return resp.text
    else:
        raise ValueError()


def parse(html):
    """ 使用xpath解析 """
    root = etree.HTML(html)  # Element 元素对象
    a_list_rows = root.xpath('//a[@class="li-row"]')
    res = []
    for a in a_list_rows:
        # 提取src的属性值
        img_url = a.xpath('.//img/@src')[0]  # list['', ]
        title = a.xpath('.//div[@class="nowrap-min li-community-title"]')[
            0].text
        price = a.xpath('.//div[@class="community-price"]//strong')[0].text
        location = a.xpath('.//div[@class="props nowrap"]//span')[2].text
        res.append((img_url, title, price, location))
    return res


html = get(url)
ret = parse(html)

print(ret)

```

## 古诗文网

- 读取页面
- 解析
- 保存到数据库内（用到了之前DAO的思想）
- 保存到csv文件（一定要掌握）

http://gushiwen.org

```python
import os
import uuid
from csv import DictWriter

import requests
from lxml import etree

url = 'https://www.gushiwen.org/'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}


def get(url):
    resp = requests.get(url, headers=headers)
    if resp.status_code == 200:
        return resp.text
    else:
        raise Exception('请求失败！')


def parse(html):
    root = etree.HTML(html)
    divs = root.xpath('//div[@class="yizhu"]/parent::*')
    for d in divs:
        item = dict()
        item['id'] = uuid.uuid4().hex
        item['title'] = d.xpath('./p[1]/a/b')[0].text
        item['author'] = d.xpath('./p[2]/a')[0].text
        item['dynasty'] = d.xpath('./p[2]/a')[1].text
        content = d.xpath('./div[@class="contson"]/child::*/text() '
                          '| ./div[@class="contson"]/text()')

        item['content'] = '<br>'.join(content)
        # itempipline(item)
        itempipline4csv(item)


def itempipline(item):
    sql = 'insert into dushuwang(%s) values(%s)'
    fields = ','.join(item.keys)
    values = ','.join(['%%(%s)s' % k for k in item])
    with conn as c:
        c.execute(sql % (fields, values), item)
    print(item)


def itempipline4csv(item):
    # 是否第一次写入csv的头
    has_header = os.path.isfile('dushuwang.csv')

    header_fields = item.keys()
    with open('dushuwang.csv', mode='a', newline='') as f:
        writer = DictWriter(f, header_fields)
        # 如果是第一次写这个文件，要写入csv的header
        if not has_header:
            writer.writeheader()
        writer.writerow(item)


html = get(url)
parse(html)

```



## 古诗文网登陆(验证码识别)

https://so.gushiwen.cn/user/login.aspx

使用这个验证码识别平台：http://chaojiying.com

我自己找到了免费的百度ocr识别

经过测试，使用[通用文字识别（高精度版）](https://cloud.baidu.com/doc/OCR/s/1k3h7y3db)可以识别出验证码

```python
# 验证码识别
# baiduocr.py
# encoding:utf-8

import requests
import base64
from PIL import Image

def get_captcha_code():
    """ 把验证码图片保存到captcha.png """
    # 通用文字识别（高精度版）的api
    request_url = "https://aip.baidubce.com/rest/2.0/ocr/v1/accurate_basic"

    # 获取access_token的url
    api_key = 'N6gDYnmH7YRikHiG9iTIv9E8'
    secret_key = 'vCZ8fW6S7brHXGFsMmGBC9s2XtolmOc6'

    host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=%s&client_secret=%s'
    host = host % (api_key, secret_key)

    access_token = requests.get(host).json()['access_token']

    im = Image.open('captcha.png')
    # 需要放大，不然会格式错误
    im = im.resize((200, 80))
    im.save('tmp.png')

    # 二进制方式打开图片文件
    with open('tmp.png', 'rb') as f:
        img = base64.b64encode(f.read())

    params = {"image": img, 'language_type': 'ENG'}
    request_url = request_url + "?access_token=" + access_token
    headers = {'content-type': 'application/x-www-form-urlencoded'}
    response = requests.post(request_url, data=params, headers=headers)
    if response:
        return response.json()['words_result'][0]['words'].strip()
```



```python
import requests

url = 'https://movie.douban.com/j/chart/top_list' # 结尾加不加'?'都行

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:85.0) '
                  'Gecko/20100101 Firefox/85.0'
}

params = {
    'type': 11,
    'interval_id': '100:90',
    'action': '',
    'start': 120,
    'limit': 20
}

resp = requests.get(url, params=params, headers=headers)
print(resp.url)
if resp.status_code == 200:
    # Content-Type: application/json; charset=utf-8
    print(resp.headers.get('Content-Type'))
    print(resp.json())
else:
    print('爬取失败, %s' % resp.status_code)
```



## 豆瓣爬虫

- 使用requests.get()
- 使用params参数

```python
import requests

url = 'https://movie.douban.com/j/chart/top_list' # 结尾加不加'?'都行

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:85.0) '
                  'Gecko/20100101 Firefox/85.0'
}

params = {
    'type': 11,
    'interval_id': '100:90',
    'action': '',
    'start': 120,
    'limit': 20
}

resp = requests.get(url, params=params, headers=headers)
print(resp.url)
if resp.status_code == 200:
    # Content-Type: application/json; charset=utf-8
    print(resp.headers.get('Content-Type'))
    print(resp.json())
else:
    print('爬取失败, %s' % resp.status_code)
```

# 总结

`requests.get()` / `requests.post()` 等方法都是基于 `requests.request()` 的

常见参数：

- `params` get()专用，用于在url后面自动拼接字符串
- `data` post()专用
- `json` 请求格式是`application/json`时使用

`requests.session()`session对象，可以调用get() / post()等方法

多次请求的会话是同一个，不需要重复传cookies，headers。



返回对象是`requests.Response`，常用属性

- status_code
- headers
- encoding
- text / content
- cookies
- json()