[toc]



# 1. 特征分类

### 分离数值\类别特征

```python
num_fea = list(data.select_dtypes(exclude='object').columns)
cat_fea = list(data.select_dtypes(include='object').columns)
```



### 分离连续\非连续数值特征

默认不同的取值数超过10个，则该特征是连续数值型特征

```python
def get_num_serial_noserial_fea(data, feas, nunique=10):
    num_serial_fea = []
    num_noserial_fea = []
    for fea in feas:
        if data[fea].nunique() > nunique:
            num_serial_fea.append(fea)
        else:
            num_noserial_fea.append(fea)
    return num_serial_fea, num_noserial_fea
```

# 2. 缺失值与异常值处理

## 2.1 缺失值

### 缺失值查看

```python
missing = data.isna().sum()
missing = missing[missing>0].sort_values()
missing_ratio = missing / len(data)  # 缺失值比例
missing.plot.barh()
```

### 缺失值处理

```python
data = data.fillna(0)  # 填0

# 用上一条数据填充缺失值
data = data.fillna(axis=0, method='ffill')  

# 用下面的值填充缺失值，且至多填充2条
data = data.fillna(axis=0, method='bfill', limit=2)

# 中位数填充
data = data.fillna(data.median())

# 众数填充
data = data.fillna(data.mode())
```

## 2.2 异常值

### 3sigma方法处理异常值

#### 查找异常值

```python
def find_outliers_by_3sigma(data, fea):  # --> series
    """ 返回一个bool series, True表示是异常值 """
    std = data[fea].std()
    mean = data[fea].mean()
    
    bottom = mean - 3 * std
    top = mean + 3 * std
    return data[fea].apply(lambda x: False if bottom <= x <= top else True)
```

#### 异常值查看

```python
n_fea = len(num_fea)
normal = []
outlier = []
for fea in num_fea:
    n = find_outliers_by_3sigma(data, fea).sum()
    normal.append(len(data) - n)
    outlier.append(n)
    
plt.figure(figsize=(6, 16))
plt.barh(range(n_fea), normal)
plt.barh(range(n_fea), outlier, left=normal)
plt.yticks(range(n_fea), num_fea)
plt.show()
```

#### 删除异常值

```python
for fea in num_fea:
    tmp = find_outliers_by_3sigma(data, fea)
    data = data[tmp]
```

### 箱线图处理异常值

#### 查找异常值

```python
def find_outliers_by_boxplot(data, fea):
    q1, q3 = data[fea].quantile(q=[0.25, 0.75])  # 获取下四， 上四分位数
    iqr = q3 - q1  # 计算四分位距
    
    # 计算上下限
    top = q3 + 1.5 * iqr
    bottom = q1 - 1.5 * iqr
    
    return data[fea].apply(lambda x: not (bottom <= x <= top))

tmp = find_outliers_by_boxplot(train, 'loanAmnt')
```



#### 绘制箱线图

```python
import matplotlib.pyplot as plt
import numpy as np

plt.boxplot(data['loanAmnt'])
plt.show()
```

#### sns绘制箱线图

http://seaborn.pydata.org/examples/grouped_boxplot.html

```python
import seaborn as sns
sns.set_theme(style="ticks", palette="pastel")

# Load the example tips dataset
tips = sns.load_dataset("tips")

# Draw a nested boxplot to show bills by day and time
sns.boxplot(x="day", y="total_bill",
            hue="smoker", palette=["m", "g"],
            data=tips)
sns.despine(offset=10, trim=True)
```

# 3. 连续值处理

## 3.1数据分箱

- 基本原则
   - 最小分箱占比不低于5%
   - 箱内不能全是好样本
   - 连续箱单调

- 为什么要分箱
   - 降低数据复杂度
   - 减少噪声影响
   - 缺失值单独作为一个分箱

### 固定宽度分箱

当数值横跨多个数量级时，最好按照 10 的幂（或任何常数的幂）来进行分组：0-9、10-99、100-999、1000-9999，等等。固定宽度分箱非常容易计算，但如果计数值中有比较大的缺口，就会产生很多没有任何数据的空箱子。

```python
x = np.linspace(1, 1000, 100)
# 通过除法映射到间隔均匀的分箱中，每个分箱的取值范围都是x/1000
np.floor_divide(x, 100)

## 通过对数函数映射到指数宽度分箱
np.floor(np.log10(x))
```



```python
x = np.linspace(1, 1000, 100)

pd.cut(x, bins=10)  # 分成10箱，数据间隔是相当的

pd.cut(x, bins=[1, 300, 800, 1000])  # 分成3箱，每两个数据夹成一箱
```



### 分位数分箱

```python
x = np.linspace(1, 1000, 100)
pd.qcut(x, q=10, labels=False)  # 分成10个箱子，每箱数据量尽量一致

pd.qcut(x, q=[0.25, 0.5, 0.75])  # 分成2箱，0-0.25和0.75-1的数据被抛弃了
```

### 查看连续箱是否单调

```python
tmp = data[['loanAmnt', 'isDefault']].copy()
tmp['loanAmnt'] = pd.qcut(tmp['loanAmnt'], q=5)

tmp.groupby(['loanAmnt'])['isDefault'].agg(['mean'])
```

## 3.2 归一化

```python
# 最大最小值归一化
data[feas] = (data[feas] - data[feas].min()) / (data[feas].max() - data[feas].min())

# 均值归一化
data[feas] = (data[feas] - data[feas].mean()) / data[feas].std()

```



# 4.类别特征处理

```python
# 具有优先级特性的可以编码成数值型特征
data['grade'] = data['grade'].map({'A':1,'B':2,'C':3,
                                   'D':4,'E':5,'F':6,'G':7})

# 转成one-hot特征
data = pd.get_dummies(data, columns=['subGrade', 'homeOwnership',
                                     'verificationStatus', 'purpose',
                                     'regionCode'], drop_first=True)
```

或是使用sklearn.preprocess中的LabelEncoder

```python
from sklearn.preprocess import LabelEncoder
for fea in cat_fea:
    le = LabelEncoder()
    data[fea] = le.fit_transform(data[fea])
```





# 5. 特征交互:star:

类别特征根据数值特征的特点进行交互

```python
def combine_cat_num_features(data, cat, num, cake):
    """
    data: DataFrame
    cat: list, feature names
    num: list, numerical feature name
    cake: list, 是用于构造新特征的指标比如 cake = ['mean', 'std', 'median']
    
    新特征的列名命名规则:
    
    cat1+cat2_num_mean
    cat_num_std
    
    由2个下划线把特征名分成「3」部分
    第1部分：表示用于组合的类别型特征，可能有一个或多个，如果是多个，用+号连接类别特征名
    第2部分：表示用于组合的数值型特征名，只有一个
    第3部分：表示用什么指标组合特征，只有一个
    
    """
    # 前缀，表示是用什么类别特征组合的
    prefix = '+'.join(cat) + '_'
    # 获得新特征的名称
    new_columns = cat + [prefix + n+'_'+c for n in num for c in cake]
    
    tmp = data.groupby(by=cat)[num].agg(cake).reset_index()
    tmp.columns = new_columns
    return tmp
```

合并到数据集

```python
tmp = combine_cat_num_features(data, cat, num, cake)
data = data.merge(tmp, on=cat)
```

# 6. 特征选择

- 特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度。特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。

特征选择的方法：

- 1 Filter
  - 方差选择法
  - 相关系数法（pearson 相关系数）
  - 卡方检验
  - 互信息法
- 2 Wrapper （RFE）
  - 递归特征消除法
- 3 Embedded
  - 基于惩罚项的特征选择法
  - 基于树模型的特征选择



## 6.1 Filter

### 方差选择法

```python
from sklearn.feature_selection import VarianceThreshold
#其中参数threshold为方差的阈值
# 只保留方差大于3的特征
VarianceThreshold(threshold=3).fit_transform(train)
```

### 相关系数法

默认peason系数计算相关度

```python
corr = data[num_fea].corr()

# 可视化
import seaborn as sns
plt.figure(figsize=(14, 14))
sns.heatmap(corr, square = True, vmin=-1, vmax=1, cmap='RdBu')
```

把相关系数高的特征过滤掉

```python
n = len(corr)
low_corr_fea = []
for i in range(n):
    for j in range(i+1, n):
        if abs(corr.iloc[i, j]) > 0.9:
            break
    else:
        low_corr_fea.append(corr.columns[i])
        
# 剩下的是两两之间相关性低于0.9的特征
```



## 6.2 Wrapper

（Recursive feature elimination，RFE）

递归特征消除法 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 在feature_selection库的RFE类可以用于选择特征，相关代码如下（以逻辑回归为例）：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数

RFE(estimator=LogisticRegression(), 
    n_features_to_select=2).fit_transform(train, target_train)
```

## 6.3 Embedded

基于惩罚项的特征选择法 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 在feature_selection库的SelectFromModel类结合逻辑回归模型可以用于选择特征，相关代码如下：