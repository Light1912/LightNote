# 特征选择

## 特征方差选择

方差越大的特征，可以认为是比较有用的。若方差较小，那么这个特征可能对我们的算法作用没有那么大。

最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对我们的模型训练没有任何作用，可以直接舍弃。

在实际应用中，我们会指定一个方差的阈值，当方差小于这个阈值的特征会被我们筛掉。sklearn中的VarianceThreshold类可以很方便的完成这个工作。

方差选择也属于过滤法的一种

## 过滤式(filter)

过滤式方法先对数据集进行特征选择，然后再训练学习器。特征选择过程与后续学习器无关，这相当于先对初始特征进行“过滤”，再用过滤后的特征训练模型。

过滤式选择的方法有：

  1. 移除低方差的特征；

  2. 相关系数排序，分别计算每个特征与输出值之间的相关系数，设定一个阈值，选择相关系数大于阈值的部分特征；

  3. 利用假设检验得到特征与输出值之间的相关性，方法有比如卡方检验、t检验、F检验等。

  4. 互信息，利用互信息从信息熵的角度分析相关性。


## 包裹式(wrapper)

包裹式从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。

包裹式特征选择直接针对给定学习器进行优化。

优点：从最终学习器的性能来看，包裹式比过滤式更好；

缺点：由于特征选择过程中需要多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择要大得多。

包裹法中，特征子集的搜索问题，最容易想到的办法是穷举法，还可以在拉斯维加斯方法框架下使用随机策略进行子集搜索（Las Vegas Wrapper，LVW）。但是由于LVW算法中特征子集搜索采用了随机策略，每次特征子集评价都需要训练学习器，计算开销很大，如果初始特征数很多，算法可能运行很长时间都达不到停止条件，若有运行时间限制，可能给不出解。

因此，我们通常使用的是贪心算法：如前向搜索（在最优的子集上逐步增加特征，直到增加特征并不能使模型性能提升为止）、后向搜索、双向搜索（将前向搜索和后向搜索相结合）。

## 嵌入式(embed)

在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。

嵌入式选择最常用的是L1正则化和L2正则化

正则化项越大，模型越简单，系数越小，当正则化项增大到一定程度时，所有的特征系数都会趋于0，在这个过程中，会有一部分特征的系数先变成0。也就实现了特征选择过程。

逻辑回归、线性回归、决策树都可以当作正则化选择特征的基学习器，只有可以得到特征系数或者可以得到特征重要度的算法才可以作为嵌入式选择的基学习器。