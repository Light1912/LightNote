# 通用流程

建立模型

选择优化器

设计训练流程：

- 定义好epoch_num, batch_size
- 计算`batch_y_pred`
- 计算`loss`
- 根据`loss`反向传播
- 更新梯度

数据特征一览

```

"""
特征       解释
-------------------------------------------
CRIM      该镇人均犯罪率
ZN        占地面积超过25,000平方尺的住宅用地比例
INDUS     非零售商业用地比例
CHAS      是否邻近Charies River
NOX       一氧化氮浓度
RM        每栋房屋平均客房数
AGE       1940年之前建成的自用单位比例
DIS       到波士顿5个就业中心的加权距离
RAD       到径向公路的可达性指数
TAX       全值财产税率
PTRATIO   学生与教师的比例
B         1000 * (BK - 0.63) ^ 2
LSTAT     低收入人群占比
MEDV      同类房屋价格的中位数

"""
```



# numpy

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_boston

"""
特征       解释
-------------------------------------------
CRIM      该镇人均犯罪率
ZN        占地面积超过25,000平方尺的住宅用地比例
INDUS     非零售商业用地比例
CHAS      是否邻近Charies River
NOX       一氧化氮浓度
RM        每栋房屋平均客房数
AGE       1940年之前建成的自用单位比例
DIS       到波士顿5个就业中心的加权距离
RAD       到径向公路的可达性指数
TAX       全值财产税率
PTRATIO   学生与教师的比例
B         1000 * (BK - 0.63) ^ 2
LSTAT     低收入人群占比
MEDV      同类房屋价格的中位数

"""


def get_dataset(batch_size=8):
    """ 因为这个简单项目主要是用作学习，所以简化了以下流程
    - 没有分训练测试集
    - 没有shuffle

    :return: 一个迭代器
    """
    dataset = load_boston()

    # x shape (n_samples, n_features)
    x, y = dataset['data'], dataset['target']
    y = y.reshape(-1, 1)

    # 归一化
    x = (x - x.mean(axis=0)) / x.std(axis=0)
    x = x.astype('float32')
    y = y.astype('float32')

    for k in range(0, len(y), batch_size):
        batch_x = x[k:k + batch_size]
        batch_y = y[k:k + batch_size]
        yield batch_x, batch_y


class LinearRegression:
    def __init__(self, n_features):
        np.random.seed(2020)
        self.w = np.random.randn(n_features, 1)
        self.b = 0.

    def forward(self, x):
        return x @ self.w + self.b

    @staticmethod
    def loss(y, y_pred):
        return 1 / len(y) * np.sum((y - y_pred) ** 2)

    def gradient(self, x, y, y_pred):
        dw = 1 / len(y) * x.T @ (y_pred - y)
        db = 1 / len(y) * np.sum(y_pred - y)
        return dw, db

    def backward(self, x, y, y_pred, learning_rate):
        dw, db = self.gradient(x, y, y_pred)
        self.w -= learning_rate * dw
        self.b -= learning_rate * db

    def gradient_descent(self, x, y, num_epochs=100, learning_rate=1e-3):
        for epoch in range(num_epochs):
            y_pred = self.forward(x)
            self.backward(x, y, y_pred, learning_rate)


def train(model, epoch_num=100, batch_size=8, learning_rate=1e-3):
    losses = []

    for epoch in range(epoch_num):
        ds = get_dataset(batch_size)
        for step, (batch_x, batch_y) in enumerate(ds):
            batch_y_pred = model.forward(batch_x)
            loss = model.loss(batch_y, batch_y_pred)

            model.backward(batch_x, batch_y, batch_y_pred, learning_rate)
            losses.append(loss)

            if (step + 1) % 10 == 0:
                print('epoch: {} step: {} loss: {:.2f}'.format(epoch, step, loss))
    return losses


model = LinearRegression(13)
losses = train(model)
ds = get_dataset()
x, y = next(ds)
# model.gradient_descent(x, y, num_epochs=100, learning_rate=1e-1)

```



# paddle

```python
import paddle
import paddle.fluid as fluid
import paddle.fluid.dygraph as dygraph
from paddle.fluid.dygraph import Linear
import numpy as np
import os
```

