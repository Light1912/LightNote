# Flask实现文件上传服务

- 简单搭建一个flask后端，使其能上传文件到后端的文件夹
- 基于requests实现上传 / 下载文件的脚本

创建`file_server`文件夹，新建以下文件夹和文件

```
file_server
  |- static
  |- __init__.py
  |- manage.py
```

在`__init__.py`中

```python
from flask import Flask

app = Flask(__name__,
            static_url_path='/s',
            static_folder='static')
```

在`manage.py`中

```python
from flask import request, jsonify
from werkzeug.datastructures import FileStorage

from file_server import app


@app.route('/upload/', methods=['POST'])
def upload():
    # 测试时通过 type(file) 得到
    file: FileStorage = request.files.get('file')
    # print(type(file), file.filename, file.content_type)
    # 保存文件到 static内
    file.save(f'static/{file.filename}')

    return jsonify({'status': 'ok',
                    'path': f'http://localhost:8000/s/{file.filename}'})


if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8000, debug=True)

```

在flask中`file`对象是`<class 'werkzeug.datastructures.FileStorage'>`

## 文件上传脚本

在任意地方创建这个脚本

```python
import requests

resp = requests.post(url='http://localhost:8000/upload/',
                     files={
                         'file': ('demo.jpg',
                                  open('demo.jpg', 'rb'),
                                  'image/jpg')
                     })
print(resp.json())
# {'path': 'http://localhost:8000/s/demo.jpg', 'status': 'ok'}
```

可以直接点击返回的链接查看图片

# 协程爬虫

> 协程是线程的替代品，区别在于线程是由CPU调度，协程由用户（程序）自己调度的。协程需要事件监听模型（事件循环器）
>
> 它采用IO多路复用原理，在多个协程之间进行调度。



## 协程的三种方式

基于生成器 generator （过渡）

- yield
- send()

python3 之后引入了 asyncio模块

- @asyncio.coroutine 协程装饰器，可以在函数上使用该装饰器，使得函数变成协程对象
- 在协程函数中，可以使用yield from 阻塞当前的协程，将执行的权限移交给yield from 之后的协程对象
- asyncio.get_event_loop() 获取事件循环模型对象，等待所有的协程对象完成之后结束

python3.5 之后，引入两个关键字

- async 替代@asynico.coroutine
- await 替代 yield from

## 协程的第三方框架

- gevent
- eventlet
- Tornado



## 例

### 协程爬取图片网1

根据之前的项目修改成协程的方式，之前的主程序

需要等待post()完成之后才能执行parse，现在可以改成

```python
if __name__ == "__main__":
    url = 'http://www.meinv.hk/wp-admin/admin-ajax.php'
    for page in range(1, 11):
        html = post(url, page)
        parse(html)
```



```python
import asyncio
import csv
import os

import requests
import time
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}
url = 'http://www.meinv.hk/wp-admin/admin-ajax.php'
filepath = 'mv.csv'


async def post(url, page=1):
    """ 使用post获取下一页请求 """
    await asyncio.sleep(1)
    resp = requests.post(url, data={
        'total': "28",
        'action': "fa_load_postlist",
        'paged': page,
        'category': "28",
        'wowDelay': "0.3s"
    }, headers=headers)
    if resp.status_code == 200:
        html = resp.json()['postlist']
        await parse(html)


async def parse(html):
    root = BeautifulSoup(html, 'lxml')
    content_box_divs = root.select('.content-box')
    for div in content_box_divs:
        item = dict()
        img = div.find('img')
        item['name'] = img['alt']
        item['cover'] = img.attrs.get('src')
        # info = div.select('.posts-text')[0].get_text()
        await itempipeline(item)


async def itempipeline(item):
    print(item)
    await save_csv(item)


async def save_csv(item):
    header = os.path.isfile(filepath)

    with open(filepath, 'a', encoding='utf8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=item.keys())
        if not header:
            writer.writeheader()
        writer.writerow(item)


async def main():
    tasks = [post(url, page) for page in range(1, 11)]
    await asyncio.gather(*tasks)


# async def main():
#     for page in range(1, 11):
#         await post('http://www.meinv.hk/wp-admin/admin-ajax.php', page)

if __name__ == "__main__":
    asyncio.run(main())
```

# 动态js渲染

## Selenium

> Selenium是驱动浏览器（chrome，firefox）进行浏览器相关操作（打开URL，点击网页中按钮连接、输入文本

需求：

- selenium库
- 相关浏览的驱动程序

见[selenium.md](selenium/selenium.md)

### 爬取智联招聘

https://www.zhaopin.com/citymap/

- 使用老方法获取每个城市的url
- 使用selenium搜索每个城市的python岗位

```python
import requests
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}
base_url = 'https://www.zhaopin.com/citymap/'


def get(url):
    resp = requests.get(url, headers=headers)
    if resp.status_code == 200:
        return resp.text


def parse_city(html):
    root = BeautifulSoup(html, 'lxml')
    links = root.select('a[class=cities-show__list--href]')
    for a in links:
        city = a.get_text().strip()
        url = 'http:' + a['href']
        yield (city, url)

if __name__ == '__main__':
    
    base_html = get(base_url)
    for city, city_url in parse_city(base_html):
        html = get(city_url)
        break
```

使用selenium，读取特定城市的岗位

```python
from selenium import webdriver

chrome = webdriver.Chrome('C:\Program Files\Google\Chrome\Application\chromedriver.exe')

url = 'https://www.zhaopin.com/shanghai/'

def get(url):
    chrome.get(url)

get(url)
```

智联招聘现在需要登录才能搜索

我可以用这个网站：

https://www.easyicon.net/iconsearch/%E5%9B%BE%E6%A0%87%E7%BD%91/

免费图标网，输入想要爬取的图标进行下载。

```python
from selenium import webdriver

chrome = webdriver.Chrome('C:\Program Files\Google\Chrome\Application\chromedriver.exe')

url = 'https://www.easyicon.net/iconsearch/%E5%9B%BE%E6%A0%87%E7%BD%91/'

def get(url):
    chrome.get(url)
    btn = chrome.find_by_css_selector('.btn btn-green bleft')
    btn.click()
get(url)

```



## Splash

> Splash 是Web服务，基于WebKit技术框架，可以动态加载网页。

# 其他

flume 日志服务器

