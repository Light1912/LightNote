# ElasticSearch搜索引擎

需要在服务器上的docker安装es

封装基于requests库操作es的函数

```python
# utils/es.py
import requests

INDEX_HOST = '119.45.58.134'
INDEX_PORT = 80

class EsIndex:
    """ Elastic Search 的索引库类 """
    def __init__(self, index_name, doc_type):
        self.index_name = index_name
        self.doc_type = doc_type
        self.url = f'http://{INDEX_HOST:INDEX_PORT}/{self.index_name}'
    # 创建索引
    def create(self):
        json_data = {
            "settings": {
                "number_of_shards": 5,
                "number_of_replicas": 1
            }
        }
        resp = requests.put(self.url, json=json_data)
        if resp.status_code == 200:
            print('create index ok!')
            print(resp.json())
    
    # 删除索引库
    def delete(self):
        resp = requests.delete(self.url)
        if resp.status_code == 200:
            print('delete index ok!')
            print(resp.json())
     
    # 向库中增加文档
    def add_doc(self, item: dict):
        doc_id = item.pop('id', None)
        if doc_id:
            url = self.url + f'/{self.doc_type}/{doc_id}'
        
        resp = requests.post(url, json=item)
        if resp.status_code == 200:
            print('add %s ok!' % url)
           
    
    # 删除文档
    def remove_doc(self, doc_id=None):
        url = self.url + f'/{self.doc_type}/{doc_id}'
        resp = requests.delete(url)
        if resp.status_code == 200:
            print('delete %s ok!' % url)
    
    # 更新文档
    def update_doc(self, id=None):
        doc_id = item.pop('id')
        url = self.url + f'/{self.doc_type}/{doc_id}'
        resp = requests.put(url, json=item)
        if resp.status_code == 200:
            print('update %s ok!' % url)
            
    
    # 查询文档
    def query(self, wd=''):
        self.url + f'/_search'
        params = {
            "size": 13,
            "from": 4,
            "q": wd
        }
        resp = requests.get(url, params=params)
        if resp.status_code == 200:
            return resp.json()
```

# 正则解析数据

http://sc.chinaz.com/tupian/shuaigetupian.html

- 由用户指定页码，实现帅哥分类图片的本地下载功能
- 保存到本地的shuaige文件夹内
- 以图片标题来对图片资源进行命名操作

```python
import re

import requests
import os
base_url = 'http://sc.chinaz.com/tupian/'
url = f'{base_url}shuaigetupian.html'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}

imgs = list()
for i in range(10):
    filename = url.split('/')[-1]
    if os.path.isfile(filename):
        with open(filename, encoding='utf-8') as f:
            html = f.read()
    else:
        resp = requests.get(url, headers=headers)
        # print(resp.encoding)  # ISO-8859-1  国际标准编码
        resp.encoding = 'utf-8'  # 可以通过这个赋值方法转换字符编码
        assert resp.status_code == 200
        html = resp.text
        with open(filename, 'w', encoding='utf8') as f:
            f.write(html)

    # 正则表达式匹配

    r = re.compile(f'<img src2="(.*?)" alt="(.*?)">')
    # r2 = re.compile(f'<img alt="(.*?)" src="(.*?)">')

    imgs.extend(r.findall(html))
    print(len(imgs))

    next_url = re.findall(r'<a href="(shuaigetupian_\d+.html)" class="nextpage">', html)

    url = base_url + next_url[0]
    print(url)
```

这个项目，主要是复习了正则表达式，其实可以用xpath再做一遍。

# bs4

## 什么是BeautifulSoup

官网：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html

BeautifulSoup，和lxml一样，是一个html的解析器
主要功能也是解析和提取数据

优点：接口设计人性化，使用方便

缺点：效率没有lxml的效率高

安装

```
pip install bs4
```

导包

```python
from bs4 import BeautifulSoup
```

创建对象

```python
# 方式1 传string
soup = BeautifulSoup(html, 'lxml')

# 方式2 打开本地文件
soup = BeautifulSoup(open('demo.html'), 'lxml')
```



## 使用属性查找

```python
soup.a  # 只能找到第一个a
```

## 使用函数查找

#### **find()**

返回符合条件的第一个标签

```python
soup.find('a')

soup.find('a', title='name')

soup.find('a', class_='student')
```

#### **find_all()**

返回一个列表

```python
find_all('a')  # 查找所有的a标签

find_all(['a', 'span'])  # 查找所有的a标签和span标签

find_all('a', limit=2)  # 只找前两个a
```

#### <font color=red>**select()**</font>

推荐

##### 选择所有指定的标签

选择所有的`<p>`标签

```python
select('p')
```

##### 根据类名选择

选择`class="animal"`的所有元素

```python
soup.select('.animal')
```

##### 根据id值选择

选择`id="apple"`的所有元素

```python
soup.select('#firstname')
```

##### 属性选择器

选择带有class属性的所有元素

```python
soup.select('li[class]')
```

选择指定属性值的元素

```python
soup.select('div[class=animal]')
```

##### 层级选择器

类似xpath中的`//`

选择 `<div>` 元素内部的所有 `<p> `元素

```python
soup.select('div p')
```

选择` <div> `元素的所有直接子` <p> `元素。

```python
soup.select('div>p')
```

选择所有` <div> `元素和所有 `<p>` 元素

```python
soup.select('div,p')
```

## 获取子孙结点

- contents：返回的是一个列表
- descendants：返回的是一个生成器

## 节点

### 节点的类型

- `bs4.BeautifulSoup` 根节点类型
- `bs4.element.NavigableString`连接类型
- `bs4.element.Tag` 节点类型
- `bs4.element.Comment` 注释类型

### 节点的内容

```python
obj.string

# 文本节点内容
obj.text

# 推荐
obj.get_text()
```

### 节点属性

```python
# 获取标签名
tag.name

# 将所有属性值作为一个字典返回
tag.attrs
```

### 获取节点的属性

```python
obj.attrs.get('title')

obj.get('title')

obj['title']
```





## 例子

糗事百科用户信息内容解析

```python
    def parse(self, html):
        soup = BeautifulSoup(html,  'lxml')
        authors = soup.select('div[class="author clearfix"]')
        print(authors)
        for author in authors:
            try:
                home = author.select('a')[0].attrs.get('href')
                id = home.split('/')[-2]
                img = author.select('img')[0]
                href = img.get('src')
                name = img.get('alt')
                age = author.select('div')[0].text
                print(id,home,href,name,age)
            except:
                pass
```



### 图片网爬虫

meinv.hk

关于如何获取下一页

- 查看Network中的XHR

```python
import requests
import time
from bs4 import BeautifulSoup

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}
url = 'http://www.meinv.hk/?cat=28'


def parse(html):
    root = BeautifulSoup(html, 'lxml')
    content_box_divs = root.select('.content-box')
    for div in content_box_divs:
        item = dict()
        img = div.find('img')
        item['name'] = img['alt']
        item['cover'] = img.attrs.get('src')
        # info = div.select('.posts-text')[0].get_text()
        itempipeline(item)


def post(url, page=1):
    """ 使用post获取下一页请求 """
    time.sleep(1)
    resp = requests.post(url, data={
        'total': "28",
        'action': "fa_load_postlist",
        'paged': page,
        'category': "28",
        'wowDelay': "0.3s"
    }, headers=headers)
    if resp.status_code == 200:
        return resp.json()['postlist']


def itempipeline(item):
    print(item)


if __name__ == "__main__":
    for page in range(1, 11):
        html = post('http://www.meinv.hk/wp-admin/admin-ajax.php', page)
        parse(html)

```



# 多任务爬虫

并行与并发，同步与异步怎么理解。

多进程 / 多线程 / 协程都可以实现异步

[IO多路复用机制详解](https://www.cnblogs.com/yanguhung/p/10145755.html)

[协程的select、poll、epoll](https://www.cnblogs.com/zhaof/p/5932461.html)

架构：

主进程：

- 下载任务队列：起始时由主进程分配首个下载任务
- 解析任务队列
- 主进程会启动下载进程和解析进程

下载进程

- 从下载任务队列取任务
- 开始下载
- 下载完成后把下载内容交给解析队列

解析进程

- 从解析队列取任务
- 开始解析数据
- **如果解析到需要继续下载的任务**，就把任务交给下载任务队列

```python
import uuid
from multiprocessing import Queue, Process
from threading import Thread
from queue import Queue as TQueue
import requests
from lxml import etree


class DownloadThread(Thread):
    def __init__(self, url):
        super().__init__()
        self.url = url
        self.content = None

    def run(self) -> None:
        print('start download: ', self.url)
        resp = requests.get(self.url, headers=headers)
        if resp.status_code == 200:
            resp.encoding = 'utf-8'
            self.content = resp.text
            print('download success!')
            return
        print('download error!')

    def get_content(self):
        return self.content


class DownloadProcess(Process):
    """ 下载进程 """

    def __init__(self, download_queue, parse_queue):
        self.download_queue = download_queue
        self.parse_queue = parse_queue
        super().__init__()

    def run(self):
        while True:
            try:
                url = self.download_queue.get(timeout=3)
                # 将下载任务交给子线程
                thread = DownloadThread(url)
                thread.start()
                thread.join()

                # 获取下载的数据
                html = thread.get_content()
                self.parse_queue.put((url, html))

            except Exception as e:
                print(e)
                break

        print('download over')


class ParseThread(Thread):
    def __init__(self, html, download_queue):
        super().__init__()
        self.html = html
        self.download_queue = download_queue

    def run(self):
        root = etree.HTML(self.html)
        imgs = root.xpath('//div[contains(@class, "picblock")]//img')
        item = {}
        # 获取下一页的链接
        next_page = root.xpath('//a[@class="nextpage"]//@href')

        if next_page:
            # 将新的url添加到下载队列中
            next_url = 'https://sc.chinaz.com/tupian/' + next_page[0]
            self.download_queue.put(next_url)

        for img in imgs:
            item['id'] = uuid.uuid4().hex
            item['name'] = img.xpath('./@alt')[0]
            try:
                item['cover'] = img.xpath('./@src2')[0]
            except:
                item['cover'] = img.xpath('./@src')[0]

            # 将item数据写入 ES 索引库中
            self.save_img(item)


    def save_img(self, item):
        filename = item['name'] + '.png'
        resp = requests.get('http:' + item['cover'], headers=headers)
        with open('./img/' + filename, 'wb') as f:
            f.write(resp.content)
        print('success save %s' % filename)


class ParseProcess(Process):
    """ 解析进程 """

    def __init__(self, download_queue, parse_queue):
        self.download_queue = download_queue
        self.parse_queue = parse_queue
        super().__init__()

    def run(self):
        while True:
            try:
                # 读取解析的任务
                url, html = self.parse_queue.get(timeout=5)
                print('start parse %s' % url)
                ParseThread(html, self.download_queue).start()
            except Exception as e:
                print(str(e))
                break

        print('parse over')


headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}

if __name__ == "__main__":
    task1 = Queue()  # 下载任务队列
    task2 = Queue()  # 解析任务队列
    # 起始爬虫任务
    task1.put('https://sc.chinaz.com/tupian/shuaigetupian.html')
    p1 = DownloadProcess(task1, task2)
    p2 = ParseProcess(task1, task2)

    p1.start()
    p2.start()

    p1.join()
    p2.join()

    print('over!')

```

### 使用线程池下载



和前一个版本相比，简化了进程的部分，就不用进程了，同时开启多个线程

```python
import uuid
from threading import Thread
import queue
import requests
from lxml import etree


class DownloadThread(Thread):
    def __init__(self, download_queue, parse_queue):
        super().__init__()
        self.download_queue = download_queue
        self.parse_queue = parse_queue

    def run(self) -> None:
        while True:
            try:
                url = self.download_queue.get(timeout=5)
                html = self.get(url)
                self.parse_queue.put((url, html))
            except queue.Empty:
                print('download finish')
                break

    @staticmethod
    def get(url):
        print('start download: ', url)
        resp = requests.get(url, headers=headers)
        if resp.status_code == 200:
            resp.encoding = 'utf-8'
            print('download success!')
            return resp.text
        print('download error!')


class ParseThread(Thread):
    def __init__(self, download_queue, parse_queue):
        super().__init__()
        self.download_queue = download_queue
        self.parse_queue = parse_queue

    def run(self):
        while True:
            try:
                url, html = self.parse_queue.get(timeout=10)
                self.parse(html)
            except queue.Empty:
                print('parse finish')
                break

    def parse(self, html):
        root = etree.HTML(html)
        imgs = root.xpath('//div[contains(@class, "picblock")]//img')
        item = {}
        # 获取下一页的链接
        next_page = root.xpath('//a[@class="nextpage"]//@href')

        if next_page:
            # 将新的url添加到下载队列中
            next_url = 'https://sc.chinaz.com/tupian/' + next_page[0]
            self.download_queue.put(next_url)

        for img in imgs:
            item['id'] = uuid.uuid4().hex
            item['name'] = img.xpath('./@alt')[0]
            try:
                item['cover'] = img.xpath('./@src2')[0]
            except IndexError:
                item['cover'] = img.xpath('./@src')[0]

            # 将item数据写入 ES 索引库中
            self.save_img(item)

    @staticmethod
    def save_img(item):
        filename = item['name'] + '.png'
        resp = requests.get('http:' + item['cover'], headers=headers)
        with open('./img/' + filename, 'wb') as f:
            f.write(resp.content)
        print('success save %s' % filename)


headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                  ' (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}

if __name__ == "__main__":
    task1 = queue.Queue()  # 下载任务队列
    task2 = queue.Queue()  # 解析任务队列
    # 起始爬虫任务
    task1.put('https://sc.chinaz.com/tupian/shuaigetupian.html')
    download_threads = [DownloadThread(task1, task2) for i in range(2)]
    parse_threads = [ParseThread(task1, task2) for i in range(4)]

    for t in download_threads:
        t.start()
    for t in parse_threads:
        t.start()

    for t in download_threads:
        t.join()

    for t in parse_threads:
        t.join()

    print('over!')

```

