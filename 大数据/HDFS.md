HDFS(Hadoop Distribute FileSystem)，

HDFS 是 Hadoop 生态系统中的分布式文件系统，它负责存储大量的数据并提供高吞吐量的数据访问。

## HDFS的设计目标

**硬件错误问题**：当一部分HDFS的组件失效时。错误检测和快速、自动的恢复是HDFS最核心的架构目标。

**流式数据访问**：运行在HDFS上的应用需要流式访问数据集，因此需要提高数据的吞吐量。

**大规模数据集**：运行在HDFS上的应用具有很大的数据集一个典型文件大小一般都在G字节至T字节。因此，HDFS被调节以支持大文件存储。它应该能提供整体上高的数据传输带宽，能在一个集群里扩展到数百个节点。一个单一的HDFS实例应该能支撑数以千万计的文件。

**简单的一致性模型**：HDFS应用需要一个“一次写入多次读取”的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。Map/Reduce应用或者网络爬虫应用都非常适合这个模型。	

**移动计算比移动数据更划算**：一个应用请求的计算，离它操作的数据越近就越高效，在数据达到海量级别的时候更是如此。因为这样就能降低网络阻塞的影响，提高系统数据的吞吐量。将计算移动到数据附近，比之将数据移动到应用所在显然更好。HDFS为应用提供了将它们自己移动到数据附近的接口。

## Namenode与Datanode

HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。

Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。

集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。

HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。

Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。

Namenode也负责确定数据块到具体Datanode节点的映射。

Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。

![HDFS 架构](images/hdfsarchitecture.gif)

> 💡**个人的疑问**
>
> **Namenode 和 Datanode 是独立的服务器吗？**：通常情况下，Namenode 和 Datanode 都是独立的服务器（或者虚拟机、容器等）。Namenode 是中心服务器，负责管理整个 HDFS 的元数据和名字空间。Datanode 则负责实际的数据存储和管理。
>
> **Namenode 相当于根服务器吗，大管家的角色？**：Namenode 是 HDFS 架构中的 "主" 节点，它负责管理和调度 Datanode 上的数据存储。它维护了文件系统树和整个树中各个文件和目录的元数据。当需要读取或写入文件时，Namenode 会决定数据块应该存储在哪些 Datanode 上，或者从哪些 Datanode 上读取。

一个典型的部署场景是一台机器上只运行一个Namenode实例，而集群中的其它机器分别运行一个Datanode实例。这种架构并不排斥在一台机器上运行多个Datanode，只不过这样的情况比较少见。

## 文件系统的命名空间

- **文件组织**：HDFS 有一个像传统文件系统一样的层次结构。你可以在里面创建目录，并在这些目录里保存文件。
- **基础操作**：你可以进行基础的文件操作，比如创建、删除、移动和重命名文件。
- **限制**：目前，HDFS 不支持磁盘配额、访问权限控制，以及硬链接和软链接。
- **Namenode 的角色**：Namenode 是管理者，负责维护这个文件系统的“名字空间”（也就是文件和目录的结构和信息）。
- **文件副本**：你可以设置一个文件在 HDFS 中应该有多少个副本（备份），这个信息也是由 Namenode 管理的。

## 数据复制

回顾一开始说的设计目的：HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。

**存储方式**：它将每个文件存储成一系列的数据块，所有的数据块都是同样大小的（除了最后一个数据块）。

**副本配置**：为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本数量可以在文件创建的时候指定，也可以在之后改变。

**一次写入**：HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。

![HDFS Datanodes](images/hdfsdatanodes.gif)

> 看了这个图我才明白，其实hadoop中数据是分散存储在各个节点的，每个数据都有备份，但是不是说每个结点都存完全一样的数据（我以前以为是这样，比如3个节点，这三个节点都存全量的数据）

### 副本存放

那副本是怎么存的呢？

HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。

**一个简单的方案**：副本存放在不同的机架上

优点：

- 可以有效防止当整个机架失效时数据丢失
- 并读数据的时充分利用多个机架的带宽
- 这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡

缺点：

- 这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。

**HDFS的策略**：

默认的副本数量是3，HDFS将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。

这种策略减少了机架间的数据传输，这就提高了写操作的效率。

机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。

于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。

在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。

> 💡什么是机架？
>
> 机架（Rack）是一个物理概念，用于描述一种数据中心内部的组织方式。在一个大型数据中心里，通常会有多个机架，每个机架上装有多台服务器（也就是节点）。这些机架通过交换机进行通信。在同一个机架内的服务器之间的通信通常比跨机架的通信要快，因为它们更接近，网络延迟更低。

### 副本选择

为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。

## 文件系统元数据的持久化

Namenode上保存着HDFS的名字空间。对于任何对文件系统元数据产生修改的操作，Namenode都会使用一种称为EditLog的事务日志记录下来。

例如，在HDFS中创建一个文件，Namenode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。Namenode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的名字空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在Namenode所在的本地文件系统上。

> 就是在HDFS中的操作都会记录到Namenode所在的服务器上的一个文件里

Namenode在内存中保存着整个文件系统的名字空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的Namenode足够支撑大量的文件和目录。当Namenode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在Namenode启动时，在不久的将来将实现支持周期性的检查点。

> 有点像docker的image，对image做操作以后把操作保存下来，形成一个新的image。

Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到Namenode，这个报告就是块状态报告。

> Datanode会把数据以文件形式存到本地文件系统，文件目录结构它自己有一套方式安排。

**Namenode**

1. **名字空间和元数据**：Namenode 在内存中保存了整个 HDFS 的名字空间（文件和目录结构）以及文件数据块的映射（Blockmap）。
  
2. **EditLog**：任何修改文件系统元数据的操作都会被记录在一个叫做 EditLog 的事务日志中。
  
3. **FsImage**：所有的名字空间和数据块到文件的映射等信息存储在一个叫做 FsImage 的文件中。
  
4. **检查点（Checkpoint）**：启动 Namenode 时，它会读取 EditLog 和 FsImage，更新内存中的 FsImage，并保存一个新版本到本地磁盘，然后删除旧的 EditLog。

**Datanode：**

1. **本地存储**：Datanode 将 HDFS 数据以文件的形式存储在本地文件系统中，每个 HDFS 数据块都是一个单独的本地文件。
  
2. **目录管理**：Datanode 不会把所有文件都存储在同一个目录下，而是会根据需要创建子目录，以优化本地文件系统的性能。
  
3. **块状态报告**：Datanode 启动时会扫描本地文件系统，生成一个所有 HDFS 数据块的列表，并将这个列表（块状态报告）发送到 Namenode。

## 健壮性

HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：

- Namenode出错,
- Datanode出错
- 网络割裂(network partitions)。

> 💡有没有这三种出错的情况的简单案例？

**Namenode 出错**

- **案例**：假设 Namenode 的硬盘发生故障，导致无法读取存储在其中的 FsImage 和 EditLog。
- **影响**：这种情况下，整个 HDFS 集群将无法正常工作，因为 Namenode 负责管理所有的文件元数据和数据块映射。
- **解决方案**：通常，会有备份的 Namenode（也称为 Secondary Namenode 或 HA Namenode）来接管，或者从最近的检查点（checkpoint）恢复。

**Datanode 出错**

- **案例**：假设某个 Datanode 的电源突然断掉。
- **影响**：存储在该 Datanode 上的数据块将暂时不可用。
- **解决**：由于 HDFS 通常会有多个副本（通常是三个），所以其他 Datanode 上的副本可以用来恢复数据。系统会自动在其他健康的 Datanode 上创建新的副本。

 **网络割裂（Network Partitions）**

- **案例**：假设数据中心的网络交换机出现问题，导致一部分 Datanode 与 Namenode 之间的通信中断。
- **影响**：这将导致 Namenode 无法访问被隔离的 Datanode 上的数据块。
- **解决方案**：在网络恢复后，Namenode 会重新同步状态，并可能触发数据块的重新复制以保证数据的可靠性。

### 磁盘数据错误

每个Datanode节点周期性地向Namenode发送心跳信号。

网络割裂可能导致一部分Datanode跟Namenode失去联系。

Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。

任何存储在宕机Datanode上的数据将不再有效。

Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。

在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。

### 集群均衡

HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。

**这些均衡策略目前还没有实现。** 

😂



### 数据完整性

从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。

HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。

当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。

> 如果某个Datanode的数据块损坏了，HDFS可以检测出来，然后可以去其他的节点去取数据。
>
> **为什么能？**当创建一个文件时，HDFS计算了文件所有数据块的校验和，并保存起来，当要读文件时，HDFS再对数据块的校验和一开始的校验进行匹配。就能发现匹配不一致的文件。

### 元数据磁盘错误

FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低Namenode每秒处理的名字空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当Namenode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。

Namenode是HDFS集群中的单点故障(single point of failure)所在。如果Namenode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做Namenode故障转移的功能还没实现。

### 快照

快照支持某一特定时刻的数据的复制备份。利用快照，可以让HDFS在数据损坏时恢复到过去一个已知正确的时间点。HDFS目前还不支持快照功能，但计划在将来的版本进行支持。

## 数据组织

### 数据块

HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是64MB。因而，HDFS中的文件总是按照64M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。

> 大文件拆分以后存储



### Staging

客户端创建文件的请求其实并没有立即发送给Namenode，事实上，在刚开始阶段HDFS客户端会先将文件数据缓存到本地的一个临时文件。应用程序的写操作被透明地重定向到这个临时文件。当这个临时文件累积的数据量超过一个数据块的大小，客户端才会联系Namenode。Namenode将文件名插入文件系统的层次结构中，并且分配一个数据块给它。然后返回Datanode的标识符和目标数据块给客户端。接着客户端将这块数据从本地临时文件上传到指定的Datanode上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的Datanode上。然后客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到日志里进行存储。如果Namenode在文件关闭前宕机了，则该文件将丢失。

上述方法是对在HDFS上运行的目标应用进行认真考虑后得到的结果。这些应用需要进行文件的流式写入。如果不采用客户端缓存，由于网络速度和网络堵塞会对吞估量造成比较大的影响。这种方法并不是没有先例的，早期的文件系统，比如AFS，就用客户端缓存来提高性能。为了达到更高的数据上传效率，已经放松了POSIX标准的要求。



### 流水线复制

当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从Namenode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。

>  	在这里，"本地" 指的是运行 HDFS 客户端的机器，而不是 Namenode 所在的服务器。当一个应用程序或用户尝试通过 HDFS 客户端创建或写入一个文件时，这些写操作最初都会被缓存到客户端机器上的一个临时文件中。



1. **性能优化**：通过先写入本地临时文件，可以减少与 Namenode 和 Datanode 的网络交互，从而提高写操作的效率。

2. **流式写入**：这种设计考虑了目标应用通常需要进行流式写入。缓存可以缓解网络速度和网络堵塞可能带来的性能影响。

3. **数据块累积**：只有当临时文件的数据量达到一个数据块的大小时，客户端才会与 Namenode 交互来获取存储该数据块的 Datanode 列表。

4. **流水线复制**：在数据块大小的数据被累积后，客户端会开始一个流水线复制过程，依次将数据发送到多个 Datanode 上。

## 可访问性

HDFS给应用提供了多种访问方式。用户可以通过[Java API](https://hadoop.apache.org/core/docs/current/api/)接口访问，也可以通过C语言的封装API访问，还可以通过浏览器的方式访问HDFS中的文件。通过WebDAV协议访问的方式正在开发中。

### DFSShell

HDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(DFSShell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例：

|                  动作                  |                  命令                  |
| :------------------------------------: | :------------------------------------: |
|      创建一个名为 /foodir 的目录       |     bin/hadoop dfs -mkdir /foodir      |
|      创建一个名为 /foodir 的目录       |     bin/hadoop dfs -mkdir /foodir      |
| 查看名为 /foodir/myfile.txt 的文件内容 | bin/hadoop dfs -cat /foodir/myfile.txt |

DFSShell 可以用在那些通过脚本语言和文件系统进行交互的应用程序上。

### DFSAdmin

DFSAdmin 命令用来管理HDFS集群。这些命令只有HDSF的管理员才能使用。下面是一些动作/命令的示例：

|              动作               |                      命令                      |
| :-----------------------------: | :--------------------------------------------: |
|       将集群置于安全模式        |      bin/hadoop dfsadmin -safemode enter       |
|        显示Datanode列表         |          bin/hadoop dfsadmin -report           |
| 使Datanode节点 datanodename退役 | bin/hadoop dfsadmin -decommission datanodename |



### 浏览器接口

一个典型的HDFS安装会在一个可配置的TCP端口开启一个Web服务器用于暴露HDFS的名字空间。用户可以用浏览器来浏览HDFS的命名空间和查看文件的内容。

## 存储空间回收

### 文件的删除和恢复

当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的，当超过这个时间时，Namenode就会将该文件从名字空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。

只要被删除的文件还在/trash目录中，用户就可以恢复这个文件。如果用户想恢复被删除的文件，他/她可以浏览/trash目录找回该文件。/trash目录仅仅保存被删除文件的最后副本。/trash目录与其他的目录没有什么区别，除了一点：在该目录上HDFS会应用一个特殊策略来自动删除文件。目前的默认策略是删除/trash中保留时间超过6小时的文件。将来，这个策略可以通过一个被良好定义的接口配置。

### 减少副本系数

当一个文件的副本系数被减小后，Namenode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。