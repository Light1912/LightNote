# 标量运算

对应元素相互运算。python常见的运算符基本都能使用

特点是**张量的维度不会变**。

支持的运算符：

| 运算           | 方法                                                  |
| -------------- | ----------------------------------------------------- |
| 基础的加减乘除 | `+` `-` `*` `/`                                       |
| 幂运算         | `**`                                                  |
| 取整           | `//`                                                  |
| 取模           | `%`                                                   |
| 关系运算       | `==` `>` `<` `>=` `<=`                                |
| 开方           | `tf.sqrt(a)`                                          |
| 多张量相加     | `tf.add_n([a, b, c])`                                 |
| 多张量求最值   | `tf.maximum([a, b, c])` <br />`tf.minimum([a, b, c])` |
| 四舍五入       | `tf.math.round(a)`                                    |
| 向下取整       | `tf.math.floor(a)`                                    |
| 向上取整       | `tf.math.ceil(a)`                                     |
| 幅值裁剪       | `tf.clip_by_value()`<br />`tf.clip_by_norm()`         |

# 向量运算

特点是基于某个轴进行运算，通常会发生**降维**。通常以`reduce`开头

| 运算                     | 方法                 |
| ------------------------ | -------------------- |
| 求和                     | `tf.reduce_sum(a)`   |
| 求均值                   | `tf.reduce_mean(a)`  |
| 求最大值                 | `tf.reduce_max(a)`   |
| 求最小值                 | `tf.reduce_min(a)`   |
| 求乘积                   | `tf.reduce_prod(a)`  |
| 是否全都是True           | `tf.reduce_all(a)`   |
| 是否有任意一个元素是True | `tf.reduce_any(a)`   |
| 最大值索引               | `tf.argmax(a)`       |
| 最小值索引               | `tf.argmin(a)`       |
| 累加                     | `tf.math.cumsum(a)`  |
| 累乘                     | `tf.math.cumprod(a)` |
| top k                    | `tf.math.top_k()`    |

## 带reduce的函数

它们的参数全是

```
tf.reduce_xx(input_tensor, axis=None, keepdims=False)
```

下面以`tf.reduce_sum()`为例，其他都是一样的

```python
import tensorflow as tf

a = tf.constant([[1, 2, 3],
                 [4, 5, 6]])
```

不带参数`axis`和`keepdims`，结果是一个标量

```python
tf.reduce_sum(a)  # 28
```

###  **指定轴axis**

竖着加

```python
tf.reduce_sum(a, axis=0)
```

```
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9])>
```

横着加

```python
tf.reduce_sum(a, axis=1)
```

```
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 6, 15])>
```

### **keepdims=True**

如果不设置这个参数，执行过求和之后，输出必定比原张量的维度低。设置该参数后，维度个数不会减少，与axis对应的维度长度降为1

```python
a = tf.ones([2, 3, 4])
tf.reduce_sum(a, axis=0, keepdims=True).shape
# TensorShape([1, 3, 4])
```
```python
tf.reduce_sum(a, axis=1, keepdims=True).shape
# TensorShape([2, 1, 4])
```

```python
tf.reduce_sum(a, axis=2, keepdims=True).shape
# TensorShape([2, 3, 1])
```

### 总结

假设一个张量维度为`[3, 4, 5]`，使用`tf.reduce_xx`过后的维度情况

|        | keepdims=False | keepdims=True |
| ------ | -------------- | ------------- |
| axis=0 | `[4, 5]`       | `[1, 4, 5]`   |
| axis=1 | `[3, 5]`       | `[3, 1, 5]`   |
| axis=2 | `[3, 4]`       | `[3, 4, 1]`   |



## 最值索引

```python
tf.argmax(input, axis=None)
tf.argmin(input, axis=None)
```

在某个轴上取最大/最小值，会使张量的维度减小一维

```python
import tensorflow as tf

a = tf.constant([[4, 1, 7, 6],
                 [3, 5, 2, 3],
                 [6, 2, 5, 8]])
```

```python
tf.argmax(a, axis=0)
# [2, 1, 0, 2]

tf.argmax(a, axis=1)
# [2, 1, 3]
```

```python
tf.argmin(a, axis=0)
# [1, 0, 1, 1]

tf.argmin(a, axis=1)
# [1, 2, 1]
```



## 累加和累乘

```
tf.math.cumprod(x, axis=0, exclusive=False, reverse=False)
tf.math.cumsum(x, axis=0, exclusive=False, reverse=False)
```

它们两个的参数基本一致

主要理解`exclusive`和`reverse`的用法

```python
import tensorflow as tf

a = tf.constant([1, 2, 3])
```

默认参数下

```python
tf.math.cumsum(a)  # [1, 1+2, 1+2+3]
# [1, 3, 6]
```

带上`exclusive`，作用就是首元素固定是0，自动忽略最后一个元素

```python
tf.math.cumsum(a, exclusive=True)  # [0, 1, 1+2]
# [0, 1, 3]
```

带上`reverse`，作用是反着加

```python
tf.math.cumsum(a, reverse=True)  # [3+2+1, 3+2, 3]
# [6, 5, 3]
```

同时带上`exclusive`和`reverse`

```python
tf.math.cumsum(a, exclusive=True, reverse=True)  # [3+2, 3, 0]
# [5, 3, 0]
```

累乘与累加基本一致，除了累乘的`exclusive`首元素固定是1

## top k

```python
tf.math.top_k(input, k=1) -> values, indices
```

```python
import tensorflow as tf
a = tf.constant([4, 2, 5, 1, 6 ,5, 3])
```

```python
values, indices = tf.math.top_k(a, k=3)
tf.print(values)  # 最大的k个值
tf.print(indices)  # 最大的k个值对应的索引
```

```
[6 5 5]
[4 2 5]
```

是多维向量的情况

```python
a = tf.constant([[3, 1, 2, 4], [4, 5, 6, 7]])
values, indices = tf.math.top_k(a, k=2)
tf.print(values)
tf.print(indices)
```

```
[[4 3]
 [7 6]]
[[3 0]
 [3 2]]
```

会依次找出每一行的top k个值和索引



# 矩阵运算

矩阵必须是二维的，会发生维度变化，可能**降维**也可能**增维**

| 运算         | 方法                                  |
| ------------ | ------------------------------------- |
| 矩阵乘法     | `a @ b` 等价于<br />`tf.matmul(a, b)` |
| 矩阵转置     | `tf.transpose(a)`                     |
| 矩阵求逆     | `tf.linalg.inv(a)`                    |
| 矩阵求迹     | `tf.linalg.trace(a)`                  |
| 矩阵求范数   | `tf.linalg.norm(a)`                   |
| 矩阵求行列式 | `tf.linalg.det(a)`                    |
| 矩阵求特征值 | `tf.linalg.eigvals(a)`                |
| 矩阵QR分解   | `tf.linalg.qr(a)`                     |
| 矩阵SVD分解  | `tf.linalg.svd(a)`                    |

# 广播机制



- **什么是广播**：如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度一致

  比如两个维度不相同的张量进行标量运算

- 两个张量在某个**维度上相容**需要满足以下条件

  - 两个张量在该维度上的**长度相同**
  - 或者其中一个张量在该维度上的**长度为1**

- **广播的条件**

  - 两张量的维度个数相同（轴的个数相同，`len(a.shape) == len(b.shape)`）
  - 两张量在所有维度上都相容

- 在广播的过程中，如果张量在某维度的长度为1，那该张量就会沿着这个维度复制自身，直到该张量在这个维度上的长度等于另一个张量在这个维度上的长度。

- 广播之后，每个维度的长度取决于较长的一方。

给出两向量维度，下面是两向量可以广播的例子：

- `(3, 1)`与`(3, 4)`
- `(2, 1, 1)`与`(2, 3, 4)`

下面是不可以广播的例子：

- `(3, )`与`(3, 4)`
- `(2, 1)`与`(2, 3, 4)`
- `(2, 1)`与`(4, 2)`

