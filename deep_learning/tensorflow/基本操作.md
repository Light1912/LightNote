这个基本操作应该是随便写出来的

- 创建一个常量$X=[0.1, 0.2, 0.3, 0.4]$
- 创建一个常量$y=[1.2,2.2,3.2,4.2]$
- 创建一个变量$w$和变量$b$，初始值为0
- 设定迭代次数`num_epoch=100`
- 创建一个SGD优化器
- 使用Tensorflow的自动求导机制，优化参数$w$和$b$，使得$\hat{y}=wx+b$尽可能接近$y$
  - 求偏导
  - 使用优化器自动更新参数
- 损失函数为

$$
L=\frac{1}{m}(\hat{y}-y)^2
$$



```python
x = tf.constant([0.1, 0.2, 0.3, 0.4])
y = tf.constant([1.2, 2.2, 3.3, 4.4])

w = tf.Variables(0.)
b = tf.Variables(0.)
variabels = [w, b]
num_epoch = 100
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)
for e in range(num_epoch):
    with tf.GradientTape() as tape:
        y_pred = x * w + b
        loss = tf.reduce_sum(tf.square(y_pred - y))
    grads = tape.gradient(loss, variabels)
    optimizer.apply_gradients(grads_and_vars=zip(grads, variabels))
```

